{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email rdb104@case.edu.<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Making a Request and Receiving a Response\n",
    "\n",
    "**Description:** This lesson introduces the basic web scraping workflow using the `requests` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 15 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** HTML Structure\n",
    "\n",
    "**Data Format:** `html`, `txt`, `py` \n",
    "\n",
    "**Libraries Used:** `requests` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to your first web scraping project.  This project will use the `requests` package to introduce the basic web scraping workflow.\n",
    "\n",
    "We will be using [Books to Scrape](https://books.toscrape.com/) as our test website for this tutorial.  The site exists just to provide a platform for people to practice web scraping.  You may want to keep the website open in a another browser tab so you can compare the data we are getting with the website as a regular user will see it. \n",
    "\n",
    "In this project you will:\n",
    "\n",
    "1. Send a request to a web server.\n",
    "2. Check for a response.\n",
    "3. View the content of that response.\n",
    "4. Save that content to a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultra Quick Jupyter Notebook Tips\n",
    "\n",
    "These Jupyter Notebooks have markdown cells, which you just read, and code cells, which you can run and edit.  \n",
    "\n",
    "Code cells appear in the light grey box.  You can click on the text in the code cell to edit it.  You can run a code cell by clicking the Run button at the top of the page (pictured) or by clicking Shift + Enter after clicking on the cell.\n",
    "\n",
    "![The Run Button](img/runcellbutton.png) \n",
    "\n",
    "Finally, all the code cells in a notebook must be run in order.  Make sure you start at the top of each lesson and run each cell in order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Requests: Sending a request and checking for a response.\n",
    "\n",
    "HTTP is a protocol for fetching resources like HTML documents, images, video, ads (yuck), and other content that make up a web page.  HTTP is a client-server protocol meaning that the client, usually a web browser, initiates a request and the server(s) sends back a response object that contains all the data.  This is where the python package `requests` gets its name, from the request piece of the request/response exchange.\n",
    "\n",
    "Web scraping doesn't use a web browser to initiate an HTTP request. We are going to use a Python script instead.  \n",
    "\n",
    "Before we can begin using a Python package, we have to import it.  Run the cell below to import the `requests` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  #https://requests.readthedocs.io/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `requests` package has been imported we can use the various excellent methods that are built into the package. The most common method for web scraping is the `get` method.\n",
    "\n",
    "`requests.get` will send a `get` request to a web address that you specify.  This simple example will get everything from the web server at that url, but `requests` has powerful tools for selecting exactly what you want to scrape, which we will explore in a later lesson.\n",
    "\n",
    "Try running the code below.\n",
    "What response do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('https://books.toscrape.com/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python sent a request to the URL we specified, https://books.toscrape.com in this case.  The response we got back is the first piece of information you need when web scraping.  Could we connect?!\n",
    "\n",
    "The 200 is one of what are called **http response status codes** and it means our attempt to connect was a success.  Excellent!\n",
    "\n",
    "A code in the 400s or 500s would mean there was an error connecting to the server.  \n",
    "\n",
    "If you want to learn more about the status codes, you can check out the developer documentation from Mozilla here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the content of a response.\n",
    "\n",
    "The status code is only one piece of information in the response object.  Now that we've made a successful connection to a website, we can start to look at the other information in the response object returned by our `get` request.  \n",
    "\n",
    "The first thing we'll need to do is store the response object in a variable using the `=` so we can look at the content without repeatedly sending requests to the web server.\n",
    "\n",
    "The code below is the same as our first `get` request, but it is stored in the variable `response`.  You could name this variable anything you wanted.  In this example we chose `results` as a shorthand for response object.  If you change the variable name here, you'll have to change it in all the following code examples as well.  So, just leave it as `results` for now.  \n",
    "\n",
    "When you're ready, run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get('https://books.toscrape.com/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice this time, you didn't get an http status code as a response.  When you store a response object in a variable, python won't display any information from the response object unless you use a command to ask it to show a piece of the response stored there. \n",
    "\n",
    "If we want to check the http status code again, we can call the variable `response` and the `status_code` attribute of the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.status_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has many other attibutes as well:\n",
    "* .headers - access header information about the server that sent the response\n",
    "* .text - access the response body as text for text-based responses, such as html, json, yaml, etc.\n",
    "* .content - access the response body as bytes for nontext requests, such as images, spreadsheets, zipfiles, etc.\n",
    "* .encoding - shows what text encoding `requests` is using in the `.text` attribute\n",
    "* .url - shows the url that was used in the request, can be useful when encoding urls with various parameters\n",
    "* .json - builtin in json decoder for scraping json files\n",
    "\n",
    "Try viewwing these different attributes by editing the code cell below and running it again to see what you get.  All the attributes follow the same format, `results` where we stored our response object, a period, the the attribute, (i.e., `text`, `headers`, `url`)\n",
    "\n",
    "For example, `results.text`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Response Object \n",
    "\n",
    "Now that we have a response object and taken a quick look at its attributes, we need to save it to a file so that we can come back to it in the future and examine it.  We could scrape the web again each time we want to do some analysis, but that would be a waste of computing resources, and the content on the web is not static.  Social media posts are deleted.  Items are removed from shops.  The format of websites changes making your web scraper obsolete.  So it is best to scrape once, and save the data locally. Saving it also allows us to easily share the data we are using with others.\n",
    "\n",
    "We are going to use a `with` statement to simplify the process of saving the response object.  The `with` statement in Python is used when you want to execute several operations as a group.  We will open the file, write the response object to it, and close the file and all these operations will happen within the `with` statement. You can learn more about `with` statments and writing to files here: https://www.freecodecamp.org/news/with-open-in-python-with-statement-syntax-example/\n",
    "\n",
    "You can see the structure of the `with` statement in the code cell below.  \n",
    "\n",
    "`with` begins the with statement.  \n",
    "`open` tells the computer to open a file using the filename you specify, in this case, `scrape.txt`, and the `w` indicates we want to write to the file.  \n",
    "`as` creates a variable that contains the file information, in this case, `file`.  \n",
    "`file.write` will save whatever attributes of the response object you include in the parentheses, (i.e., `results.text`, `results.headers`, `results.url`)\n",
    "\n",
    "The filename, `scrape.txt`, and the attribute can be changed to create different files with diffferent content.  \n",
    "\n",
    "When you run the code below you will see the file appear in the list of directories/files on the left.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the filename below to `scrapeheaders.txt` and the attribute below to `response.headers` and run the code cell again to create another file.  Did you get a differnt file with different content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
