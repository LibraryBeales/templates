{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email rdb104@case.edu.<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Scraping Data from Multiple Pages\n",
    "\n",
    "**Description:** This lesson introduces the basic web scraping workflow using the `requests` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 30 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** HTML Structure\n",
    "\n",
    "**Data Format:** `html`, `txt`, `py` \n",
    "\n",
    "**Libraries Used:** `requests` `BeautifulSoup`\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from Multiple Pages\n",
    "\n",
    "In this project you will:\n",
    "1. Use the `Inspect` tool to explore how <a href=\"https://books.toscrape.com/\">Books to Scrape</a> handles navigation between pages.\n",
    "2. Understand and use a python script to direct the web scraper to a navigate to the next page if there is one, and scrape the specified data from there, and repeat this process until there are no more pages.\n",
    "3. Examine the data to see if our scraper worked the way we think it should.\n",
    "4. Write the data to a csv file. \n",
    "\n",
    "Let's import the packages we need so we can get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests  #https://requests.readthedocs.io/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple pages using `requests`.\n",
    "\n",
    "We're going to be using much of the same code we used in the last lesson, as the data we are trying to collect is the same.  However, we are going to wrap that code in some navigational instructions for Python to use so it can visit and scrape all the pages.   \n",
    "\n",
    "If you scroll to the bottom of the page and right-click>inspect the Next button you will see the html for the next page.  \n",
    "\n",
    "![title](img/next.png)\n",
    "\n",
    "We can see that each page has a url that ends in page-NUMBER.html  We are going to create several variables that we can use to pice toether the url for each page we want to scrape. \n",
    "\n",
    "`base_url` will be the part of the url that doesn't change.  \n",
    "`page_number` will start at 1 and after we scrape a page we can add 1 to the `page_number` and use that new number to create the next url.  \n",
    "\n",
    "Run the code cell below to create both of these variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://books.toscrape.com/catalogue/'  \n",
    "\n",
    "page_number = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a `while True` statement, a loop construct that will continue executing its block of code indefinitely as long as the condition provided to it is always True. It creates an infinite loop unless a specific condition within the loop causes it to break or exit.\n",
    "\n",
    "In the context of web scraping, using `while True`` allows you to continuously scrape multiple pages by repeatedly fetching and parsing the content of subsequent pages until a specific condition is met.  In our example, the loop keeps fetching and scraping pages until it encounters a condition where there's no \"Next\" link or an error occurs while fetching the page, causing the loop to break and exit. \n",
    "\n",
    "1. while True: creates an infinite loop because the condition provided (True) is always true.\n",
    "2. Inside the loop, a URL for the current page is constructed based on the page_number.\n",
    "3. The script attempts to fetch the page content using requests.get().\n",
    "4. If the HTTP response status code is 200 (indicating a successful request), the HTML content of the page is parsed using BeautifulSoup, and the necessary scraping operations are performed.\n",
    "5. Within the loop, there's usually a condition check to determine whether to update the page_number for the next iteration or to break the loop (for example, if there's no \"Next\" link or any other condition that signifies the end of the scraping process).\n",
    "6. If the response status code is not 200, it could mean there was an error fetching the page, so the loop breaks.\n",
    "\n",
    "next_link = soup.find('li', class_='next'): This line uses Beautiful Soup (soup) to find an <li> element with the class 'next'. After using the `Inspect` tool, we know this is how \n",
    "if next_link:: This line checks if next_link contains a valid result. If next_link is not None, it means that a link to the next page has been found.\n",
    "\n",
    "page_number += 1: If a valid link to the next page is found, this line adds 1 to the page_number variable, so the script will visit a new page on the next iteration of the loop.\n",
    "\n",
    "else: break: If no 'Next' link is found on the current page (i.e., next_link is None), the script executes the break statement. This breaks out of the while True loop, effectively stopping the scraping process because there are no more pages to scrape.\n",
    "\n",
    "Overall, this section of code is responsible for checking if a 'Next' link exists on the current page. If it does, it updates the page_number variable to move to the next page for scraping. If there's no 'Next' link, the loop breaks, terminating the scraping process as it indicates that there are no more pages to scrape.\n",
    "\n",
    "Run the code block below and see if you get what you expect.  It is scraping info for 1000 books. so it may take up to 60 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store scraped data\n",
    "book_info_list = []\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    url = f'{base_url}page-{page_number}.html'\n",
    "    \n",
    "    # Fetch the page content\n",
    "    results = requests.get(url)\n",
    "    \n",
    "    if results.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        \n",
    "        # Find all articles with class 'product_pod'\n",
    "        articles = soup.find_all('article', class_='product_pod')\n",
    "        \n",
    "        # Scraping logic for each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title = article.find('h3').find('a')['title']\n",
    "\n",
    "            # Extract product price\n",
    "            price = article.find('p', class_='price_color').text.replace(\"Ã‚\", \"\")\n",
    "            \n",
    "            # Extract star rating (if available)\n",
    "            rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "            # Extract stock status\n",
    "            stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "            # Store the information in a list\n",
    "            book_info = [title, price, rating, stock]\n",
    "\n",
    "            # Append the book information to the main list\n",
    "            book_info_list.append(book_info)\n",
    "        \n",
    "        # Find the 'Next' link using the class of the li element.  \n",
    "        next_link = soup.find('li', class_='next')\n",
    "        \n",
    "        if next_link:\n",
    "            # Update page number for the next iteration\n",
    "            page_number += 1\n",
    "        else:\n",
    "            # No 'Next' link found, exit the loop\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}. Status code: {results.status_code}\")\n",
    "        break\n",
    "\n",
    "# Print the scraped data to see if it worked.\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good!!  We are getting more than the 20 books on the first page, and the data is still clean as a result of all the changes we made in project #3.\n",
    "\n",
    "Let's write it to a file so we can use it for some data visualization in Project 5.\n",
    "\n",
    "We're going to use the same csv library and `with open` command that we did in Project #3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('all_book_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(book_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explore how to use the results of our web-scraping in data visualization, go back to the starting page and take a look at Project #5.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
