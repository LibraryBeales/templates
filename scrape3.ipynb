{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email rdb104@case.edu.<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Scraping Sets of Related Information and Saving it to a CSV file.\n",
    "\n",
    "**Description:** This lesson explores how to collect multiple connected data points from a web page and store them in a csv file.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 30 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** HTML Structure\n",
    "\n",
    "**Data Format:** `html`, `txt`, `py` \n",
    "\n",
    "**Libraries Used:** `requests` `BeautifulSoup` `csv`\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project #3: Scraping sets of related information into CSV files.\n",
    "\n",
    "Build a scraper that collects multiple data points about each book based upon specific criteria.\n",
    "\n",
    "In this project you will:\n",
    "1. Determine what data we are able to collect about each book listed in the store.  \n",
    "2. Use the `Inspect` tool in your web browser to identify the web page struture for those pieces of data. \n",
    "3. Understand and use a python script to crawl the web page and extract only the data that meets the classification criteria we identified in steps 1 and 2.\n",
    "4. Write the data to a csv file. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data is available?\n",
    "\n",
    "If we look at this screenshot captured from the web page, we can see that there are several intresting pieces of data in addition to the title.  There is the price, the rating, and whether or not the book is in stock.  We could also save the cover images or collect the links to those images, but let's leave those out for right now.  So we are going to try to collect four pieces of data for each book; title, price, rating, stock status.\n",
    "\n",
    "![title](img/booklisting.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get started the same way we have in the last few lessons, importing packages! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests  #https://requests.readthedocs.io/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just like before, use requests to get the content of the website, store it in a variable, and then use BeautifulSoup to parse that content into the \"soup\" we can analyze.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Fetch the page\n",
    "results = requests.get(\"https://books.toscrape.com/\")\n",
    "\n",
    "# 2.Get the page content and assign it to the varaible 'content'\n",
    "content = results.text\n",
    "\n",
    "# 3. Create the soup\n",
    "soup = BeautifulSoup(content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the html structure of the page so we can determine how we can identify each piece of information to scrape.  Right click on a book and use the `Inspect` option to open up the Inspector panel and take a look at the html.  You may need to expand the html using the grey arrows at the beginning of the elements in order to see all the relevant information.  It shold look like the image below.\n",
    "\n",
    "Each book's information is presenteed in an `article` element with the `class=product_pod`.  We can use `find_all` to find all of these `article` elements and then scrape the data we need from each one.  \n",
    "\n",
    "`articles = soup.find_all('article', class_='product_pod')`\n",
    "\n",
    "As we determinded in the last lesson, the title is contained in the `h3` element.  \n",
    "\n",
    "`title = article.find('h3').find('a')['title']`\n",
    "\n",
    "We can see the price data is contained in a `p` element with `class=price_color`.    \n",
    "\n",
    "`price = article.find('p', class_='price_color').text`\n",
    "\n",
    "The rating is contained in another `p` element with `class=\"star-rating NUMBER\"`.  We don't want the whole `p` element, just the `class`, so we add `['class']` to this line to limit what we scrape.\n",
    "\n",
    "`rating = article.find('p', class_='star-rating')['class']`\n",
    "\n",
    "For stock status, we are using the same apporach of identifying the element by its `class`.  \n",
    "\n",
    "`stock = article.find('p', class_='instock availability').text`\n",
    "\n",
    "\n",
    "![title](img/htmlscrape.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create an empty list to store all this data in.  \n",
    "\n",
    "This line initializes an empty list called book_info_list. `book_info_list = []`\n",
    "\n",
    "Then we use a `for loop` to go through the data for each book and scrape the data we need.  We scrape title, price, rating, and stock and assign them to variables of the same name. \n",
    "\n",
    "`book_info = [title, price, rating, stockstatus]` initializes a list with the four fields of data that were collected for a book and that list is added to the `book_info_list` at the end of the loop with the `append` method `book_info_list.append(book_info)`. \n",
    "\n",
    "So we end up with a list of lists.  Each book has a list of data.  And the `book_info_list` is a list of all those lists.\n",
    "\n",
    "Run the code block below and take a look at the results to see the list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text\n",
    "\n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class']\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!  We got a lot of useful information.  However, we can see that there are some problems with the data that we are scraping that need to be cleaned up.  \n",
    "\n",
    "First, the price has an unusual `Â` character at the beginning.  This character is appearing as a result of a text encoding error.  We can add a .`replace` method to the line of code that scrapes the price.  `replace()` takes two arguments.  The first is the character you want to replace.  The second is what you want to replace it with.  So if we edit that line of code to ad replace we can take the `Â` and replace it with nothing, represented by the empty qutation marks `\"\"`.\n",
    "\n",
    "`price = article.find('p', class_='price_color').text.replace(\"Â\", \"\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the rating piece contains too much information.  `['star-rating', 'Two']`  We don't need the first part of the data, only the number in the second part.  The `[]` indicate that this is a list.  It is a short list with only two items.  We can just indicate which item we'd like to scrape using the index of the list.  In python, counting starts at zero.  So the first item in the list would be `[0]` and the second item in the list would be `[1]`.  So if we just add this index number to our line of code, we can only get the second item in the list from the `p` element. \n",
    "\n",
    "`rating = article.find('p', class_='star-rating')['class'][1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, the stock status information We are also using `.strip()` on the text.  You can specify which leading and trailing characters you'd like to remove from a string. In our case, we've left the parentheses empty so the strip method will use the default argument, which is just to remove any white space.  The `\\n` characters are line breaks and will also be deleted as white space.\n",
    "\n",
    "`stock = article.find('p', class_='instock availability').text.strip()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add all these changes to the original blick of code and we should get a much cleaner set of data.  Run the nexy code block and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text.replace(\"Â\", \"\")\n",
    "    \n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright!  That is much cleaner data. \n",
    "\n",
    "When you ahve a list of lists it is easy to store it in a csv, or comma spearated value, file.  The list of data about each book would be a new line in the csv file.\n",
    "\n",
    "First we import the `csv` package so python can work with csv files.  Then we use the same `with` statement to handle creating and writing to the file.  The `writerows` method of `csv.writer` will write the each list in our list of lists to the csv file as a new line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('book_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(book_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the `book_data.csv` file in the file directory on the left to open it and take a look at our exported data. \n",
    "\n",
    "You might notice that we only scraped the data for 20 books.  There are a thousand books on this website, but we only scraped data from the first page.  In the next tutorial we will learn how to scrape all the pages on the [Books to Scrape](https://books.toscrape.com/) website.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
